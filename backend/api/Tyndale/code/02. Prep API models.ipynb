{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from duckdb import query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 500\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these files are the output of Tyndale/code/01. Tyndale txt to pkl.py\n",
    "# which takes the txt files as its input in Tyndale/data\n",
    "# i'm going to join them all together and clean things up, add some new columns, etc.\n",
    "# then make pkl files for django models\n",
    "a = pd.read_pickle(\"../data/tagnt.pkl\")\n",
    "b = pd.read_pickle(\"../data/strongs.pkl\")\n",
    "c = pd.read_pickle(\"../data/tegmc.pkl\")\n",
    "d = pd.read_pickle(\"../data/bcvIndex.pkl\")\n",
    "\n",
    "# lexnalt data from biblehub\n",
    "e = pd.read_pickle('../../pickles/lexnalt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what's in each table, define fields to keep going forward\n",
    "print('tagnt columns: ',a.columns)\n",
    "print('strongs columns: ',b.columns)\n",
    "print('tegmc columns: ',c.columns)\n",
    "print('bcvIndex columns: ',d.columns)\n",
    "tagnt_fields = ['bcv','greek','english', 'strongs_id', 'morphology_id',]\n",
    "strongs_fields = ['strongs', 'lexicon', 'gloss', 'transliteration','definitionhtml',]\n",
    "tegmc_fields = ['morphology','function', 'tense','voice','mood','person','case','gender','number'] \n",
    "bcvIndex_fields = ['bcv','book','bookLong']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## not perfect, but forcing strongs ids to join 1-to-1 to strongs table\n",
    "## some of the words had two or more strongs ids associated with them,\n",
    "## so i'm keeping the first one in that case\n",
    "a['strongsidlength'] = a.strongs_id.apply(lambda x: len(x))\n",
    "print(a.groupby(['strongsidlength']).count().reset_index()[['strongsidlength','greek']])\n",
    "a['strongs_id'] = a.strongs_id.apply(lambda x: x.split('+')[0] if len(x) > 6 else x)\n",
    "a.sort_values('strongsidlength',ascending=False)\n",
    "a['strongsidlength'] = a.strongs_id.apply(lambda x: len(x))\n",
    "print(a.groupby(['strongsidlength']).count().reset_index()[['strongsidlength','greek']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixlist = ['CONJ +P-1NS', 'PRT-N +PRT', 'CONJ +ADV', 'CONJ +I-NSN', 'CONJ +D-NPN', 'PRT-N +ADV', 'CONJ +COND', 'CONJ +D-APN',\n",
    "'CONJ +D-ASM', 'PREP+N-ASN', 'CONJ +D-NPM', 'CONJ +P-1DS', 'CONJ +D-NSM', 'A-NPM-NUI +A-NPM-NUI', 'A-APN-NUI +A-APN-NUI',\n",
    "'CONJ +PRT +PRT', 'CONJ +P-1AS', 'ADV +V-AAP-NSM', 'CONJ +PRT', 'CONJ +D-APM', 'COND +CONJ', 'PREP +ADV', 'PRT +ADV', 'ADV +N-NPF']\n",
    "\n",
    "a['morphology_id'] = a.morphology_id.apply(\n",
    "    lambda x: 'CONJ' if x in fixlist and x[:4]=='CONJ' else\n",
    "              'CONJ' if x in fixlist and x[:4]=='COND' else\n",
    "              'PRT' if x in fixlist and x[:3]=='PRT' else\n",
    "              'PREP' if x in fixlist and x[:4]=='PREP' else\n",
    "              'A-NPM-NUI' if x in fixlist and x[:9]=='A-NPM-NUI' else\n",
    "              'A-APN-NUI' if x in fixlist and x[:9]=='A-APN-NUI' else\n",
    "              'ADV' if x in fixlist and x[:3]=='ADV' else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join tables\n",
    "df = a[a.nestleAland==True][tagnt_fields]\\\n",
    "    .reset_index(drop=True)\\\n",
    "    .rename(columns={'strongs_id':'strongs', 'morphology_id':'morphology'})\\\n",
    "    .merge(b[strongs_fields], how='left', on='strongs')\\\n",
    "    .merge(c[tegmc_fields], how='left', on='morphology')\\\n",
    "    .merge(d[bcvIndex_fields], how='left', on='bcv').fillna('')\n",
    "print(\"shape: \", df.shape)\n",
    "df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming these to xxxxxx2 because i'm going to update each field subsequently\n",
    "df = df.rename(columns={\n",
    "    'function': 'function2',\n",
    "    'tense': 'tense2',\n",
    "    'voice': 'voice2',\n",
    "    'mood': 'mood2',\n",
    "    'person': 'person2',\n",
    "    'case': 'case2',\n",
    "    'gender': 'gender2',\n",
    "    'number': 'number2',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add some ID columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build some id columns\n",
    "bcv = list(df.bcv)\n",
    "bcvw = ['01010101']\n",
    "i = 1\n",
    "bcv_ = '010101'\n",
    "for item in bcv[1:]:\n",
    "    if item == bcv_:\n",
    "        i += 1\n",
    "    else:\n",
    "        i = 1\n",
    "    w = ('0' + str(i))[-2:]\n",
    "    bcvw.append(item+w)\n",
    "    bcv_ = item\n",
    "\n",
    "df['bcvw'] = bcvw\n",
    "df['bk'] = df.bcvw.apply(lambda x: x[:2])\n",
    "df['ch'] = df.bcvw.apply(lambda x: x[2:4])\n",
    "df['vs'] = df.bcvw.apply(lambda x: x[4:6])\n",
    "df['wd'] = df.bcvw.apply(lambda x: x[6:8])\n",
    "df['bc'] = df['bk'] + df['ch']\n",
    "\n",
    "id_columns = ['book','bcvw','bcv','bc','bk','ch','vs','wd']\n",
    "df = df[id_columns + [col for col in list(df.columns) if col not in id_columns]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the word freq for each strongs number\n",
    "strongs_count = query(f\"\"\"\n",
    "SELECT \n",
    "    strongs,\n",
    "    lexicon,\n",
    "    gloss,\n",
    "    transliteration,\n",
    "    count(*) as lexicon_freq_nt \n",
    "FROM df\n",
    "GROUP BY 1,2,3,4\n",
    "ORDER BY 1,2,3,4\n",
    "\"\"\").to_df().sort_values(['lexicon_freq_nt','strongs'],ascending=[False,True]).reset_index(drop=True).reset_index().rename(columns={'index':'lexicon_id_int'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strongs_count.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(strongs_count[['strongs', 'lexicon_freq_nt', 'lexicon_id_int']], how='left', on='strongs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lexicon_id'] = df.lexicon_id_int.apply(lambda x: ('000'+str(x))[-4:] )\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redefine/Collapse/Simplify some variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['function'] = df.function2.apply(lambda x:\n",
    "    'ADJ' if x in ['Adjective',] else\n",
    "    'ADV' if x in ['Adverb', 'Adverb or adverb and particle combined',] else\n",
    "    'ART' if x in ['Definite article',] else\n",
    "    'CONJ' if x in ['Conjunction'] else\n",
    "    'PRON' if x in [\n",
    "        'Correlative or Interrogative pronoun',\n",
    "        'Correlative pronoun',\n",
    "        'Demonstrative pronoun',\n",
    "        'Indefinite pronoun',\n",
    "        'Interrogative pronoun',\n",
    "        'Personal pronoun',\n",
    "        'Possessive pronoun',\n",
    "        'Reciprocal pronoun',\n",
    "        'Reflexive pronoun',\n",
    "        'Relative pronoun',\n",
    "    ] else\n",
    "    'PRT' if x in ['Interrogative Particle', 'Negative Particle', 'Particle or Disjunctive','Interjection',] else\n",
    "    'PREP' if x in ['Preposition',] else\n",
    "    'NOUN' if x in ['Noun',] else\n",
    "    'V' if x in ['Verb',] \n",
    "    else 'OTHER'    \n",
    ")\n",
    "df.groupby(['function','function2']).count()[['wd']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(\"select function, count(*) as count from df group by 1 order by count desc\").to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## at one point, was attempting to make consistent functions by lexicon_id, but not sure that's desired on frontend\n",
    "\n",
    "# a = (df.groupby(['lexicon_id','function']).count().reset_index()[['lexicon_id','function', 'wd']]).groupby('lexicon_id').count().reset_index()\n",
    "# dups = list(a[a.wd>0].lexicon_id)\n",
    "# dups\n",
    "# a = df[df.lexicon_id.isin(dups)].groupby(['lexicon_id','function']).count().reset_index()[['lexicon_id', 'function', 'wd']]\n",
    "# b = a.groupby('lexicon_id').max().reset_index()[['lexicon_id','wd']]\n",
    "# b['flag'] = 1\n",
    "# c = a.merge(b,how='left', on=['lexicon_id', 'wd'])\n",
    "# d = c[c.flag==1].drop(columns=['flag','wd',]).rename(columns={'function':'function_new'})\n",
    "\n",
    "# temp = df.merge(d, how='left', on='lexicon_id')\n",
    "# e = temp[(temp.function != temp.function_new)].sort_values('lexicon_id', ascending=False)\n",
    "# e.groupby(['lexicon_id','function_new']).count()\n",
    "\n",
    "# temp[temp.lexicon_id.isin(['1254', '3195', '3485'])]\n",
    "# temp['function_final'] = temp.apply(lambda x: 'ADV' if x.strongs=='G1622' else 'ADV' if x.strongs=='G2115' else '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['case'] = df.case2.apply(lambda x:\n",
    "'N' if x == 'Nominative' else\n",
    "'G' if x == 'Genitive' else\n",
    "'D' if x == 'Dative' else\n",
    "'A' if x == 'Accusative' else\n",
    "'V' if x == 'Vocative' else x\n",
    ")\n",
    "df.groupby(['case','case2']).count()[['wd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tense'] = df.tense2.apply(lambda x:\n",
    "'P' if x in ['Present',] else\n",
    "'F' if x in ['Future', '2nd Future',] else\n",
    "'I' if x in ['Imperfect',] else\n",
    "'R' if x in ['Perfect', '2nd Perfect', 'Pluperfect', '2nd Pluperfect',] else\n",
    "'A' if x in ['Aorist',] else\n",
    "'2A' if x in ['2nd Aorist',]\n",
    "else x)\n",
    "df.groupby(['tense','tense2']).count()[['wd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'] = df.gender2.apply(lambda x: 'F' if x=='Feminine' else 'M' if x=='Masculine' else 'N' if x=='Neuter' else x)\n",
    "df.groupby(['gender', 'gender2']).count()[['wd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['person'] = df.person2.apply(lambda x: \n",
    "'1' if x in ['1st', '1st Plural', '1st Singular',] else \n",
    "'2' if x in ['2nd', '2nd Singular'] else\n",
    "'3' if x in ['3rd']\n",
    "else x)\n",
    "df.groupby(['person', 'person2']).count()[['wd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['voice'] = df.voice2.apply(lambda x:\n",
    "'A' if x in ['Active'] else\n",
    "'M' if x in ['Middle', 'Middle Deponent',] else\n",
    "'P' if x in ['Passive', 'Middle or Passive', 'Passive Deponent', 'Middle or Passive Deponent',]\n",
    "else x\n",
    ")\n",
    "df.groupby(['voice', 'voice2']).count()[['wd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mood'] = df.mood2.apply(lambda x:\n",
    "'I' if x in ['Indicative','Optative',] else\n",
    "'M' if x in ['Imperative',] else\n",
    "'S' if x in ['Subjunctive',]\n",
    "else x\n",
    ")\n",
    "df.groupby(['mood', 'mood2']).count()[['wd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number'] = df.number2.apply(lambda x: 'S' if x == 'Singular' else 'P' if x == 'Plural' else x)\n",
    "df.groupby(['number', 'number2']).count()[['wd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['bookLong', 'book', 'bcvw', 'bcv', 'bc', 'bk', 'ch', 'vs', 'wd',\n",
    "       'greek', 'english', 'strongs', 'lexicon_id', 'lexicon_id_int', 'morphology', 'lexicon', 'gloss', 'transliteration',\n",
    "       'function', 'tense', 'voice', 'mood', 'person', 'case', 'gender', 'number', 'lexicon_freq_nt', 'definitionhtml',]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand parsing breakouts, and Add a new Parsing field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.function.isin(['ADV', 'PREP', 'CONJ', 'PRT', 'OTHER'])].groupby(['function','tense','voice','mood','person','case','gender','number']).count()['wd'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.function.isin(['ART', 'ADJ', 'NOUN'])].groupby(['function','tense','voice','mood','person','case','gender','number']).count()['wd'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.function=='PRON'].groupby(['function','tense','voice','mood','person','case','gender','number']).count()['wd'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign mood for infinitives & participles\n",
    "df['mood'] = df.apply(lambda x: 'N' if (x.function=='V') and (x.mood=='') and (x.person=='') and (x.case=='') and (x.gender=='') and (x.number=='') else x.mood, axis=1)\n",
    "df['mood'] = df.apply(lambda x: 'P' if (x.function=='V') and (x.case != \"\") else x.mood, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.function=='V'].groupby(['function','tense','voice','mood','person','case','gender','number']).count()['wd'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parsing\n",
    "df['parsing'] = df.apply(lambda x:\n",
    "    (x.function + '-' + x.case + x.gender + x.number) if x.function in ['ART', 'ADJ', 'NOUN'] else\n",
    "    (x.function + '-' + x.case + x.gender + x.number) if ((x.function=='PRON') & (x.person not in ['1', '2', '3'])) else\n",
    "    (x.function + '-' + x.person + x.number + '-' + x.case + x.gender) if x.function=='PRON' else\n",
    "    (x.function + '-' + x.tense + x.voice + x.mood) if x.mood == 'N' else\n",
    "    (x.function + '-' + x.tense + x.voice + x.mood + '-' + x.case + x.gender + x.number) if x.mood == 'P' else\n",
    "    (x.function + '-' + x.tense + x.voice + x.mood + '-' + x.person + x.number) if x.function == 'V'    \n",
    "    else x.function\n",
    ",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsing = df.groupby(['parsing']).count()['wd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['bcv', 'bookLong', 'book', 'bk', 'ch', 'vs', 'wd', 'greek', 'english', 'parsing', 'lexicon', 'gloss', 'transliteration','lexicon_freq_nt', 'strongs', 'definitionhtml','lexicon_id', 'lexicon_id_int', 'function', 'tense', 'voice', 'mood', 'person', 'case', 'gender', 'number']]\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split word and punctuation for Greek and English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: this function can return all characters for a language, \n",
    "# but using it right now to get unique characters that end\n",
    "# a word, so I can separate out the puncuation characters\n",
    "def get_unique_characters(var):\n",
    "    word_string=''\n",
    "    for word in df[var].to_list():\n",
    "        word_string+=word[-1]\n",
    "    unique_letters = []\n",
    "    for letter in word_string:\n",
    "        if letter not in unique_letters:\n",
    "            unique_letters.append(letter)\n",
    "    return sorted(unique_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_chars = get_unique_characters('greek')\n",
    "print(greek_chars[:])\n",
    "english_chars = get_unique_characters('english')\n",
    "print(english_chars[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determined these lists from the output above, will use in next step\n",
    "greek_puncs = [',', '.', '¶', 'ͅ', ';', '·',]\n",
    "english_puncs = [' ', '!', '\"', ',', '.', ':', ';', '?',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_without_puncuation = []\n",
    "greek_puncuation = []\n",
    "for word in list(df['greek']):    \n",
    "    if len(word)>=3 and word[-3] in greek_puncs and word[-2] in greek_puncs and word[-1] in greek_puncs:            \n",
    "        greek_puncuation.append(word[-3:])\n",
    "        greek_without_puncuation.append(word[:-3])\n",
    "    elif len(word)>=2 and word[-2] in greek_puncs and word[-1] in greek_puncs:            \n",
    "        greek_puncuation.append(word[-2:])\n",
    "        greek_without_puncuation.append(word[:-2])\n",
    "    elif word[-1] in greek_puncs:            \n",
    "        greek_puncuation.append(word[-1:])\n",
    "        greek_without_puncuation.append(word[:-1])        \n",
    "    else:\n",
    "        greek_puncuation.append('')\n",
    "        greek_without_puncuation.append(word)\n",
    "\n",
    "df['greek2'] = greek_without_puncuation\n",
    "df['greek_punc'] = greek_puncuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_without_puncuation = []\n",
    "english_puncuation = []\n",
    "for word in list(df['english']):    \n",
    "    if len(word)>=3 and word[-3] in english_puncs and word[-2] in english_puncs and word[-1] in english_puncs:            \n",
    "        english_puncuation.append(word[-3:])\n",
    "        english_without_puncuation.append(word[:-3])\n",
    "    elif len(word)>=2 and word[-2] in english_puncs and word[-1] in english_puncs:            \n",
    "        english_puncuation.append(word[-2:])\n",
    "        english_without_puncuation.append(word[:-2])\n",
    "    elif word[-1] in english_puncs:            \n",
    "        english_puncuation.append(word[-1:])\n",
    "        english_without_puncuation.append(word[:-1])        \n",
    "    else:\n",
    "        english_puncuation.append('')\n",
    "        english_without_puncuation.append(word)\n",
    "\n",
    "df['english2'] = english_without_puncuation\n",
    "df['english_punc'] = english_puncuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'greek2': 'greek', 'greek':'greek2', 'english2':'english', 'english':'english2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verses = list(df['bcv'].unique())\n",
    "verse_dict = {v:i for i,v in enumerate(verses,1)}\n",
    "df['vs_id'] = df.bcv.apply(lambda x: verse_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\n",
    " 'vs_id',\n",
    " 'bcv',\n",
    " 'bookLong',\n",
    " 'book',\n",
    " 'bk',\n",
    " 'ch',\n",
    " 'vs',\n",
    " 'wd',\n",
    " 'greek',\n",
    " 'english',\n",
    " 'parsing',\n",
    " 'lexicon',\n",
    " 'lexicon_freq_nt',\n",
    " 'strongs',\n",
    " 'definitionhtml',\n",
    " 'lexicon_id',\n",
    " 'lexicon_id_int',\n",
    " 'gloss',\n",
    " 'transliteration',\n",
    " 'function',\n",
    " 'tense',\n",
    " 'voice',\n",
    " 'mood',\n",
    " 'person',\n",
    " 'case',\n",
    " 'gender',\n",
    " 'number', \n",
    " 'greek_punc',\n",
    " 'english_punc',]]\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Paradigms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping by parsing fields\n",
    "# because of various accents, keeping max count when duplicates occur\n",
    "# note: lexicon_id_int parameter is ordering of strongs ids by word frequency, so 0 is the article bc it's most common\n",
    "def get_paradigms(lexicon_id_int):  \n",
    "    fields = ['lexicon_id', 'lexicon_id_int', 'parsing',]\n",
    "    w = df[df.lexicon_id_int==lexicon_id_int]\n",
    "    w['greek'] = w.greek.apply(lambda x: x.lower())\n",
    "    x = w.groupby(fields+['greek']).count().reset_index()[fields+['greek', 'wd']].reset_index()\n",
    "    x['wd2'] = x['wd'] + x['index']/10000\n",
    "    y = x.groupby(fields).max().reset_index()[fields+['wd2']]\n",
    "    y2 = x.groupby(fields).sum().reset_index()[fields+['wd']]\n",
    "    y['flag'] = 1\n",
    "    z = x.merge(y, how='left', on=fields+['wd2']).drop(columns=['wd','wd2',])\n",
    "    z = z[z.flag==1].drop(columns=['flag',])\n",
    "    z = z.merge(y2, how='left', on=fields).drop(columns=['index', 'lexicon_id_int']).rename(columns={'wd':'paradigm_freq_nt'})\n",
    "\n",
    "    # run this to see what decisions are made in this function --> z,d = get_paradigms(18)\n",
    "    # print(w.shape[0])\n",
    "    # print(z.wd.sum())\n",
    "    \n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = get_paradigms(18)\n",
    "z.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step takes about 45sec\n",
    "lexicon_ids_int = sorted(list(df['lexicon_id_int'].unique()))\n",
    "dfs = []\n",
    "for lexicon_id_int in lexicon_ids_int:\n",
    "    dfs.append(get_paradigms(lexicon_id_int))\n",
    "pdgm = pd.concat(dfs).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that worked...\n",
    "# the difference should be exactly zero, but note decisions made for duplicate parsing records in fxn above\n",
    "fields = ['lexicon_id', 'parsing',]\n",
    "rows_in_gnt = df.shape[0]\n",
    "paradigm_word_count = pdgm.groupby(fields).sum().paradigm_freq_nt.sum()\n",
    "print(rows_in_gnt - paradigm_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create models for database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paradigm model\n",
    "# pdgm['pdgm_id'] = pdgm.lexicon_id + '_' + pdgm.parsing\n",
    "pdgm = pdgm.rename(columns={'lexicon_id': 'pdgm_lexn_id', 'parsing':'pdgm_pars_id', 'greek':'pdgm_greek', 'paradigm_freq_nt':'pdgm_freq_nt'})\n",
    "pdgm['pdgm_id'] = pdgm.pdgm_lexn_id + \"_\" + pdgm.pdgm_pars_id\n",
    "pdgm = pdgm[['pdgm_id', 'pdgm_greek', 'pdgm_freq_nt', 'pdgm_lexn_id', 'pdgm_pars_id',]]\n",
    "pdgm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing model\n",
    "parsing_fields = ['parsing', 'function', 'tense', 'voice','mood', 'person', 'case', 'gender', 'number']\n",
    "pars = df.groupby(parsing_fields).count()['wd'].reset_index().sort_values('parsing').rename(columns={'wd':'parsing_freq_nt'})\n",
    "pars = pars.rename(columns={'parsing':'pars_id', 'function':'pars_function', 'tense':'pars_tense', 'voice':'pars_voice', 'mood':'pars_mood', 'person':'pars_person', 'case':'pars_case', 'gender':'pars_gender', 'number':'pars_number', 'parsing_freq_nt':'pars_freq_nt'})\n",
    "\n",
    "# sorting for frontend to order paradigms easier\n",
    "cols = list(pars.columns)\n",
    "rank_function = {'NOUN':1, 'PRON':2, 'ADJ':3, 'V':4, 'ART':5, 'ADV':6, 'CONJ':7, 'PREP':8, 'PRT':9}\n",
    "rank_tense = {'P':1, 'I':2, 'F':3, 'A':4, '2A':5, 'R':6, '':7}\n",
    "rank_voice = {'A':1, 'M':2, 'P':3, '':4}\n",
    "rank_mood = {'I':1, 'M':2, 'N':3, 'S':4, 'P':5, '':6}\n",
    "rank_person = {'1':1, '2':2, '3':3, '':4}\n",
    "rank_case = {'N':1, 'G':2, 'D':3, 'A':4, 'V':5, '':6}\n",
    "rank_gender = {'M':1, 'F':2, 'N':3, '':4}\n",
    "rank_number = {'S':1, 'P':2, '':3}\n",
    "\n",
    "pars['rank_function'] = pars.pars_function.apply(lambda x: rank_function[x])\n",
    "pars['rank_tense'] = pars.pars_tense.apply(lambda x: rank_tense[x])\n",
    "pars['rank_voice'] = pars.pars_voice.apply(lambda x: rank_voice[x])\n",
    "pars['rank_mood'] = pars.pars_mood.apply(lambda x: rank_mood[x])\n",
    "pars['rank_person'] = pars.pars_person.apply(lambda x: rank_person[x])\n",
    "pars['rank_case'] = pars.pars_case.apply(lambda x: rank_case[x])\n",
    "pars['rank_gender'] = pars.pars_gender.apply(lambda x: rank_gender[x])\n",
    "pars['rank_number'] = pars.pars_number.apply(lambda x: rank_number[x])\n",
    "\n",
    "pars = pars.sort_values(by=[\n",
    "    'rank_function',\n",
    "    'rank_mood',\n",
    "    'rank_tense',\n",
    "    'rank_voice', \n",
    "    'rank_number',   \n",
    "    'rank_person',    \n",
    "    'rank_gender',\n",
    "    'rank_case'\n",
    "])\n",
    "\n",
    "pars = pars.drop(columns=['rank_function', 'rank_tense', 'rank_voice', 'rank_mood', 'rank_person', 'rank_case',\n",
    "'rank_gender', 'rank_number']).reset_index(drop=True).reset_index().rename(columns={\n",
    "    'index':'pars_rank'\n",
    "})\n",
    "\n",
    "pars = pars[cols + ['pars_rank']]\n",
    "\n",
    "pars.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexicon model\n",
    "lexicon_fields = ['lexicon_id', 'lexicon', 'gloss', 'transliteration', 'lexicon_freq_nt','strongs', 'definitionhtml',]\n",
    "lexn = df.groupby(lexicon_fields).count()['wd'].reset_index().sort_values('lexicon_id').drop(columns=['wd',])\n",
    "\n",
    "lexn = lexn.rename(columns={'lexicon_id':'lexn_id', 'lexicon':'lexn_greek', 'gloss':'lexn_gloss', \n",
    "'transliteration':'lexn_transliteration', 'lexicon_freq_nt':'lexn_freq_nt', 'strongs':'lexn_strongs', 'definitionhtml':'lexn_dictionaryhtml'})\n",
    "\n",
    "lexn['lexn_strongs'] = lexn.lexn_strongs.apply(lambda x: x[1:])\n",
    "temp = pdgm[['pdgm_lexn_id', 'pdgm_pars_id']].rename(columns={'pdgm_lexn_id':'lexn_id'})\n",
    "temp['lexn_function'] = temp.pdgm_pars_id.apply(lambda x: x.split('-')[0])\n",
    "temp2 = temp.groupby(['lexn_id',]).max().reset_index()[['lexn_id', 'lexn_function']]\n",
    "\n",
    "lexn = lexn.merge(temp2, how='left', on='lexn_id')\n",
    "\n",
    "# join lexnalt data from biblehub\n",
    "temp = pd.read_pickle('../../pickles/lexnalt.pkl')\n",
    "# temp['lexn_strongs'] = temp.strongs.apply(lambda x: ('000'+str(x))[-4:])\n",
    "\n",
    "lexn = lexn.merge(temp, how='left', on='lexn_strongs')\n",
    "\n",
    "lexn['greek'] = lexn.greek.fillna('')\n",
    "lexn['definition'] = lexn.definition.fillna('')\n",
    "lexn['usage'] = lexn.usage.fillna('')\n",
    "lexn['lexn_greek_long'] = lexn.apply(lambda x: x.greek if x.greek != '' else x.lexn_greek, axis=1)\n",
    "lexn['lexn_definition'] = lexn.apply(lambda x: x.definition if x.definition != '' else x.lexn_gloss, axis=1)\n",
    "lexn['lexn_usage'] = lexn.apply(lambda x: x.usage if x.usage != '' else x.lexn_gloss, axis=1)\n",
    "lexn['lexn_definition'] = lexn.lexn_definition.apply(lambda x: x.split('NAS Exhaustive')[0].split(\"Thayer's Greek\")[0])\n",
    "lexn['lexn_usage'] = lexn.lexn_usage.apply(lambda x: x.split('NAS Exhaustive')[0].split(\"Thayer's Greek\")[0][:-1])\n",
    "lexn = lexn[[\n",
    "    'lexn_id',\n",
    "    'lexn_greek',\n",
    "    'lexn_greek_long',\n",
    "    'lexn_transliteration',\n",
    "    'lexn_gloss',\n",
    "    'lexn_definition',    \n",
    "    'lexn_usage',    \n",
    "    'lexn_strongs',\n",
    "    'lexn_function',\n",
    "    'lexn_freq_nt',\n",
    "]]\n",
    "lexn.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_id'] = df.bcv + df.wd\n",
    "df['word_book_id'] = df.bk\n",
    "df['word_chap_id'] = df.bk + df.ch\n",
    "df['word_vers_id'] = df.bcv\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# book model\n",
    "book = df.groupby(['bk', 'book', 'bookLong']).count()['wd'].reset_index()\n",
    "num_chapters_per_book = df.groupby(['bk']).max()['ch'].reset_index().rename(columns={'ch':'book_num_chapters'})\n",
    "temp = df.groupby(['bk','ch','vs']).count()['wd'].reset_index()\n",
    "num_verses_per_book = temp.groupby(['bk']).count()['wd'].reset_index().rename(columns={'wd':'book_num_verses'})\n",
    "book = book\\\n",
    "        .merge(num_chapters_per_book, how='left', on='bk')\\\n",
    "        .merge(num_verses_per_book, how='left', on='bk')\\\n",
    "        .rename(columns={'bk':'book_id', 'bookLong':'book_name', 'book':'book_name_abbrev', 'book_num_chapters':'book_num_chapters', 'book_num_verses':'book_num_verses', 'wd':'book_num_words'})\n",
    "\n",
    "book = book[['book_id', 'book_name', 'book_name_abbrev', 'book_num_chapters', 'book_num_verses', 'book_num_words']]   \n",
    "book\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word model\n",
    "word = df[['word_id', 'greek', 'english', 'greek_punc', 'english_punc', 'parsing', 'lexicon_id', 'word_book_id', 'word_chap_id', 'word_vers_id', ]]\n",
    "word = word.rename(columns={'greek':'word_greek', 'english':'word_english', 'greek_punc':'word_greek_punc', 'english_punc':'word_english_punc', 'parsing':'word_pars_id', 'lexicon_id':'word_lexn_id', })\n",
    "word['word_chap_num'] = word.word_chap_id.apply(lambda x: int(x[2:]))\n",
    "word['word_vers_num'] = word.word_vers_id.apply(lambda x: int(x[4:]))\n",
    "word['word_word_num'] = word.word_id.apply(lambda x: int(x[6:]))\n",
    "word = word.merge(book, how='left', left_on='word_book_id', right_on='book_id')\\\n",
    "    .rename(columns={'book_name_abbrev':'word_book_name_abbrev',})\n",
    "word[\"word_lexn_id_copy\"] = word.word_lexn_id    \n",
    "word[\"word_pars_id_copy\"] = word.word_pars_id    \n",
    "word = word[[\n",
    " 'word_id',\n",
    " 'word_greek',\n",
    " 'word_english',\n",
    " 'word_greek_punc',\n",
    " 'word_english_punc',\n",
    " 'word_book_name_abbrev',\n",
    " 'word_chap_num',\n",
    " 'word_vers_num',\n",
    " 'word_word_num',\n",
    " 'word_lexn_id_copy',\n",
    " 'word_pars_id_copy',\n",
    " 'word_pars_id',\n",
    " 'word_lexn_id',\n",
    " 'word_book_id',\n",
    " 'word_chap_id',\n",
    " 'word_vers_id', \n",
    " \n",
    " ]]\n",
    "word.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapter model\n",
    "chap = df.groupby('word_chap_id').count()['wd'].reset_index().rename(columns={'wd':'chap_num_words'})\n",
    "temp = df.groupby(['word_chap_id', 'vs']).count()['wd'].reset_index()\n",
    "num_verses_per_chapter = temp.groupby('word_chap_id').count()['wd'].reset_index().rename(columns={'wd':'chap_num_verses'})\n",
    "chap = chap.merge(num_verses_per_chapter, how='left', on='word_chap_id').rename(columns={'word_chap_id':'chap_id'})\n",
    "chap['chap_num'] = chap.chap_id.apply(lambda x: int(x[2:]))\n",
    "chap['chap_book_id'] = chap.chap_id.apply(lambda x: x[:2])\n",
    "\n",
    "chap.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verse model\n",
    "vers = df.groupby('word_vers_id').count()['wd'].reset_index().rename(columns={'word_vers_id':'vers_id', 'wd':'vers_num_words'})\n",
    "print(vers.vers_num_words.sum())\n",
    "vers['vers_chap_num'] = vers.vers_id.apply(lambda x: int(x[2:4]))\n",
    "vers['vers_num'] = vers.vers_id.apply(lambda x: int(x[4:]))\n",
    "vers['vers_book_id'] = vers.vers_id.apply(lambda x: x[:2])\n",
    "vers['vers_chap_id'] = vers.vers_id.apply(lambda x: x[:4])\n",
    "vers = vers.merge(book, how='left', left_on='vers_book_id', right_on='book_id')\n",
    "vers['vers_ref_abbrev'] = vers.apply(lambda x: x.book_name_abbrev + '.' + \" \" + str(x.vers_chap_num) + \":\" + str(x.vers_num), axis=1)\n",
    "vers['vers_ref'] = vers.apply(lambda x: x.book_name + \" \" + str(x.vers_chap_num) + \":\" + str(x.vers_num), axis=1)\n",
    "vers['vers_chap_url'] = vers.apply(lambda x: x.book_name_abbrev + \"-\" + str(x.vers_chap_num) + \"#verse\" + str(x.vers_num), axis=1)\n",
    "vers = vers.rename(columns={'book_name_abbrev':'vers_book_name_abbrev'})\n",
    "vers = vers[[\n",
    "    'vers_id',\n",
    "    'vers_ref',\n",
    "    'vers_ref_abbrev',\n",
    "    'vers_chap_url',\n",
    "    'vers_book_name_abbrev',\n",
    "    'vers_chap_num',\n",
    "    'vers_num',\n",
    "    'vers_num_words',\n",
    "    'vers_book_id',\n",
    "    'vers_chap_id',\n",
    "]]\n",
    "vers.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the freq of each lexn word in each chapter\n",
    "temp = query(f\"\"\"\n",
    "SELECT\n",
    "    concat(word_lexn_id,'_',word_chap_id) as frlc_id, \n",
    "    word_lexn_id as frlc_lexn_id,\n",
    "    word_book_id as frlc_book_id,\n",
    "    word_chap_id as frlc_chap_id,\n",
    "    count(*) as frlc_count\n",
    "FROM word\n",
    "GROUP BY 1,2,3,4\n",
    "ORDER BY 1,2,3,4\n",
    "\"\"\").to_df()\n",
    "\n",
    "frlc = temp\\\n",
    "    .merge(chap, how='left', left_on='frlc_chap_id', right_on='chap_id')\\\n",
    "    .merge(book, how='left', left_on='frlc_book_id', right_on='book_id')\n",
    "frlc = frlc.rename(columns={'book_name_abbrev': 'frlc_book_name_abbrev', 'chap_num':'frlc_chap_num'})\n",
    "frlc = frlc[['frlc_id', 'frlc_book_name_abbrev', 'frlc_chap_num', 'frlc_count', 'frlc_lexn_id',]]\n",
    "frlc[frlc.frlc_lexn_id==\"0100\"].sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the freq of each lexn word in each book\n",
    "temp = query(f\"\"\"\n",
    "SELECT\n",
    "    concat(word_lexn_id,'_',word_book_id) as frlb_id, \n",
    "    word_lexn_id as frlb_lexn_id,\n",
    "    word_book_id as frlb_book_id,    \n",
    "    count(*) as frlb_count\n",
    "FROM word\n",
    "GROUP BY 1,2,3\n",
    "ORDER BY 1,2,3\n",
    "\"\"\").to_df()\n",
    "\n",
    "frlb = temp.merge(book, how='left', left_on='frlb_book_id', right_on='book_id')\n",
    "frlb = frlb.rename(columns={'book_name_abbrev': 'frlb_book_name_abbrev', })\n",
    "frlb = frlb[['frlb_id', 'frlb_book_name_abbrev', 'frlb_count', 'frlb_lexn_id',]]\n",
    "frlb[frlb.frlb_lexn_id==\"0100\"].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all into one table\n",
    "gnt = word\\\n",
    "        .merge(vers, how='left', left_on=\"word_vers_id\", right_on=\"vers_id\")\\\n",
    "        .merge(chap, how='left', left_on=\"word_chap_id\", right_on=\"chap_id\")\\\n",
    "        .merge(book, how='left', left_on=\"word_book_id\", right_on=\"book_id\")\\\n",
    "        .merge(pars, how='left', left_on=\"word_pars_id\", right_on=\"pars_id\")\\\n",
    "        .merge(lexn, how='left', left_on=\"word_lexn_id\", right_on=\"lexn_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pickles for django models\n",
    "word.to_pickle('../../pickles/word.pkl')\n",
    "pars.to_pickle('../../pickles/pars.pkl')\n",
    "lexn.to_pickle('../../pickles/lexn.pkl')\n",
    "pdgm.to_pickle('../../pickles/pdgm.pkl')\n",
    "book.to_pickle('../../pickles/book.pkl')\n",
    "chap.to_pickle('../../pickles/chap.pkl')\n",
    "vers.to_pickle('../../pickles/vers.pkl')\n",
    "frlc.to_pickle('../../pickles/frlc.pkl')\n",
    "frlb.to_pickle('../../pickles/frlb.pkl')\n",
    "gnt.to_pickle('../../pickles/gnt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step takes about 2.75 minutes\n",
    "with pd.ExcelWriter('../../../api/pickles/gnt.xlsx') as writer:  \n",
    "    word.to_excel(writer, sheet_name='word')\n",
    "    pars.to_excel(writer, sheet_name='pars')\n",
    "    lexn.to_excel(writer, sheet_name='lexn')\n",
    "    pdgm.to_excel(writer, sheet_name='pdgm')\n",
    "    book.to_excel(writer, sheet_name='book')\n",
    "    chap.to_excel(writer, sheet_name='chap')\n",
    "    vers.to_excel(writer, sheet_name='vers')\n",
    "    frlc.to_excel(writer, sheet_name='frlc')\n",
    "    frlb.to_excel(writer, sheet_name='frlb')\n",
    "    gnt.to_excel(writer, sheet_name='gnt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "249b2d0bd31f9fffee9a432bfa8ef53d594a6b6748a49d8f718f94d6a97b8acd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
